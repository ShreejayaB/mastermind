{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent_performance(secret='1234'):\n",
    "    '''return num of guesses needed to get secret for an random agent'''\n",
    "    random_agent = Agent()\n",
    "    random_agent.reset_possible_states()\n",
    "    guess = random_agent.get_best_action()\n",
    "    env = Environment(secret)\n",
    "    num_guess = 1\n",
    "    while guess!= secret:\n",
    "        random_agent.possible_states.remove(guess)\n",
    "        guess = random_agent.get_best_action()\n",
    "        num_guess += 1\n",
    "    return num_guess\n",
    "\n",
    "def random_agent_average_performance(num):\n",
    "    l = []\n",
    "    for _ in range(num):\n",
    "        secret = Environment._number_from_index(random.randint(0, 6**4-1))\n",
    "        length = random_agent_performance(secret)\n",
    "        l.append(length)\n",
    "    return sum(l)/len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584.2"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_agent_average_performance(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1296 possibile patterns in the mastermind game. An agent which choose random action, takes around ~ **584** guesses to match the target secret code. This random agent does not take feedback into consideration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using random agent as the baseline which will use the Five Guess algorithm to compare the performace of our Q-learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five guess algorithm solves the mastermind game with 5 or less number of guesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Agent using Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, n_episodes):\n",
    "    '''\n",
    "    Train the agent for n_episodes.\n",
    "    '''\n",
    "    for _ in range(n_episodes):\n",
    "        secret = Environment._number_from_index(random.randint(0, 6**4 - 1))\n",
    "        env = Environment(secret)\n",
    "        agent.reset_possible_states()\n",
    "        action = agent.random_action()  # init action\n",
    "        \n",
    "        if action == secret: # if init guess is crt skip this episode\n",
    "            continue\n",
    "            \n",
    "        run = True\n",
    "        while run:\n",
    "            feedback = env.get_feedback(action)\n",
    "            reward   = env.reward(action)\n",
    "            agent.learn_from_move(action, feedback, reward)\n",
    "            \n",
    "            if action == secret:\n",
    "                break  # correct guess stop episode\n",
    "            else:\n",
    "                action = agent.random_action()  # else next guess\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_guesses(agent, secret='1234'):\n",
    "    '''return number of guesses needed by the agent to\n",
    "    get to the secret'''\n",
    "    agent.reset_possible_states()\n",
    "    guess = agent.get_best_action()\n",
    "    env = Environment(secret)\n",
    "    num_guess = 1\n",
    "    while guess!= secret:\n",
    "        feedback = env.score(secret, guess)\n",
    "        agent.restrict_possible_states(guess, feedback)\n",
    "        guess = agent.get_best_action()\n",
    "        num_guess += 1\n",
    "    return num_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_num_guesses_needed(agent):\n",
    "    '''average number guesses needed for all the possible secret codes'''\n",
    "    \n",
    "    nums = [] \n",
    "    \n",
    "    for idx in range(6**4):\n",
    "        secret = Environment._number_from_index(idx)\n",
    "        length = num_guesses(agent, secret)\n",
    "        nums.append(length)\n",
    "        \n",
    "#     print(nums)\n",
    "    return sum(nums)/len(nums)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worst_case_length(agent):\n",
    "    '''num of guesses needed in worst case'''\n",
    "    \n",
    "    nums = [] \n",
    "    \n",
    "    for idx in range(6**4):\n",
    "        secret = Environment._number_from_index(idx)\n",
    "        length = num_guesses(agent, secret)\n",
    "        nums.append(length)\n",
    "        \n",
    "#     print(nums)\n",
    "    return max(nums)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train for 2000 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent = Agent(epsilon=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(q_agent, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing worst case scenario with 5-guess algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worst_case_length(q_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('learned_q_agent.pkl','wb') as f:\n",
    "    pickle.dump(q_agent, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q-learning with epsilon = 0.1 worst case performance = 8\n",
      "q-learning with epsilon = 0.2 worst case performance = 7\n",
      "q-learning with epsilon = 0.5 worst case performance = 7\n",
      "q-learning with epsilon = 0.7 worst case performance = 7\n"
     ]
    }
   ],
   "source": [
    "for epsilon in [0.1, 0.2, 0.5, 0.7]:\n",
    "    q_agent = Agent(epsilon=epsilon)\n",
    "    train(q_agent,2000)\n",
    "    wc_perf = worst_case_length(q_agent)\n",
    "    print(f'q-learning with epsilon = {epsilon} worst case performance = {wc_perf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select epsilon = 0.2 based on the hyperparameter tuning here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with Q-learning, Policy gradient method was also experimented. Enivonment and neural network for the policy gradient method can be found in the [Policy-gradient](Policy-gradient.ipynb) notebook.\n",
    "\n",
    "It was observed learning in case of policy gradient method to be very slow. In order to capture the temporal aspect between the different patterns in the game, an LSTM model was chosen to learn on the embeddings of guesses. This meant simulating more data, leading to slow.\n",
    "\n",
    "Future idea to make Policy gradient faster:\n",
    "1. Parallelize the learning process.\n",
    "2. Use GRU instead of LSTM.\n",
    "3. Simulate the data efficiently.\n",
    "4. Decrease the size of embedding of state and action vectors.\n",
    "\n",
    "Keeping the timeline in mind efforts were focussed on Q-learning due it's speed and ease of implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning algorithm seems to get closer to the popular 5-guess algorithm by Donlad knuth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(x=['5-guess','Q-learning agent'], height=[5,7])\n",
    "_ = plt.ylabel('worst case number of guesses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially Q-learning algorithm was unstable without restricting the number of states based on the feedback obtained during the game. This is due to the dynamic environment in case of mastermind. The Environment, feedback and the reward depends on the random secret code selected at the start of the game.\n",
    "\n",
    "But when state space is restricted State-values learnt by the Q-learning algorithm look to be stable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning:\n",
    "\n",
    "1. Alternative representation of action. Action can be represented using different colors at the 4 positions based on the previous guesses and feedbacks.\n",
    "2. Learn another Q-learning agent to select the secret code to make the learrning faster for the code breaking Q-learning agent.\n",
    "3. Double Q_learning to reduce the maximisation bias and speed up Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Gradient:\n",
    "\n",
    "1. Parallelize the learning process and use GPU.\n",
    "2. Use GRU instead of LSTM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://github.com/egeromin/mastermind/\n",
    "2. https://github.com/brianspiering/rl-course/tree/master/labs/lab_4_tic_tac_toe\n",
    "3. https://ieeexplore.ieee.org/document/7926576\n",
    "4. researchgate.net/publication/316947563_Playing_Mastermind_Game_by_Using_Reinforcement_Learning\n",
    "1. https://en.wikipedia.org/wiki/Mastermind_(board_game)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
